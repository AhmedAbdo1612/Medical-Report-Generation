{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install kaggle \nimport os\n\nos.environ['KAGGLE_USERNAME'] = 'ahmed010abdo178'\nos.environ['KAGGLE_KEY'] = '5a0d8902f9c4fb39b43f4d4849082f4a'\n!kaggle kernels output ahmed010abdo178/gpt-vit2 -p /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:36:42.544849Z","iopub.execute_input":"2023-10-24T01:36:42.545200Z","iopub.status.idle":"2023-10-24T01:36:43.929832Z","shell.execute_reply.started":"2023-10-24T01:36:42.545170Z","shell.execute_reply":"2023-10-24T01:36:43.928734Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"401 - Unauthorized\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd \nimport cv2\nimport numpy as np\nimport os\nfrom glob import glob\nimport math\nimport matplotlib.pyplot as plt\nimport re\nimport html\nimport string\nimport unicodedata\nfrom nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:29:25.849867Z","iopub.execute_input":"2023-10-24T20:29:25.850139Z","iopub.status.idle":"2023-10-24T20:29:28.858165Z","shell.execute_reply.started":"2023-10-24T20:29:25.850114Z","shell.execute_reply":"2023-10-24T20:29:28.856499Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"df2 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv')\ndf1 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:29:48.471460Z","iopub.execute_input":"2023-10-24T20:29:48.471802Z","iopub.status.idle":"2023-10-24T20:29:48.583525Z","shell.execute_reply.started":"2023-10-24T20:29:48.471777Z","shell.execute_reply":"2023-10-24T20:29:48.582600Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"images_captions_df = pd.DataFrame({'image': [],\n                                    'caption': []})\nfor i in range(len(df2)):\n    uid = df2.iloc[i]['uid']\n    image = df2.iloc[i]['filename']\n    index = df1.loc[df1['uid'] ==uid]\n    \n    if not index.empty:    \n        index = index.index[0]\n        caption = df1.iloc[index]['findings']\n        if type(caption) == float:\n         \n            continue \n        images_captions_df = pd.concat([images_captions_df, pd.DataFrame([{'image': image, 'caption': caption}])], ignore_index=True)\nimages_captions_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:29:55.918382Z","iopub.execute_input":"2023-10-24T20:29:55.918731Z","iopub.status.idle":"2023-10-24T20:30:05.268392Z","shell.execute_reply.started":"2023-10-24T20:29:55.918706Z","shell.execute_reply":"2023-10-24T20:30:05.267418Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                    image                                            caption\n0  1_IM-0001-4001.dcm.png  The cardiac silhouette and mediastinum size ar...\n1  1_IM-0001-3001.dcm.png  The cardiac silhouette and mediastinum size ar...\n2  2_IM-0652-1001.dcm.png  Borderline cardiomegaly. Midline sternotomy XX...\n3  2_IM-0652-2001.dcm.png  Borderline cardiomegaly. Midline sternotomy XX...\n4  4_IM-2050-1001.dcm.png  There are diffuse bilateral interstitial and a...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1_IM-0001-4001.dcm.png</td>\n      <td>The cardiac silhouette and mediastinum size ar...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1_IM-0001-3001.dcm.png</td>\n      <td>The cardiac silhouette and mediastinum size ar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2_IM-0652-1001.dcm.png</td>\n      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2_IM-0652-2001.dcm.png</td>\n      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4_IM-2050-1001.dcm.png</td>\n      <td>There are diffuse bilateral interstitial and a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def remove_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef remove_punctuation(text):\n \n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n \n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    #words = text2words(text)\n    #stop_words = stopwords.words('english')\n    #words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    #words = lemmatize_words(words)\n    #words = lemmatize_verbs(words)\n\n    return text\n  \ndef normalize_corpus(corpus):\n    return [normalize_text(t) for t in corpus]","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:37:59.939558Z","iopub.execute_input":"2023-10-24T01:37:59.939898Z","iopub.status.idle":"2023-10-24T01:37:59.954689Z","shell.execute_reply.started":"2023-10-24T01:37:59.939871Z","shell.execute_reply":"2023-10-24T01:37:59.953650Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# import swifter\nnew_df = images_captions_df.copy()\nnew_df = new_df.drop(index = range(5175,6469))\n\nval_df = images_captions_df.copy()\nval_df = val_df.drop(index = range(0,5175))\nprint(val_df.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:38:43.808173Z","iopub.execute_input":"2023-10-24T01:38:43.808579Z","iopub.status.idle":"2023-10-24T01:38:43.821158Z","shell.execute_reply.started":"2023-10-24T01:38:43.808549Z","shell.execute_reply":"2023-10-24T01:38:43.820144Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"                          image  \\\n5175  3216_IM-1520-2001.dcm.png   \n5176  3217_IM-1520-1002.dcm.png   \n5177  3218_IM-1520-1001.dcm.png   \n5178  3218_IM-1520-3001.dcm.png   \n5179  3218_IM-1520-4001.dcm.png   \n\n                                                caption  \n5175  Heart size is normal. No pneumothorax, pleural...  \n5176  Irregularity within the right apex is consiste...  \n5177  XXXX XXXX and lateral chest examination was ob...  \n5178  XXXX XXXX and lateral chest examination was ob...  \n5179  XXXX XXXX and lateral chest examination was ob...  \n","output_type":"stream"}]},{"cell_type":"code","source":"p = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\nnew_df['image'] = p+ new_df['image']\nnew_df.head()\nval_df['image'] = p+ val_df['image']","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:38:48.086170Z","iopub.execute_input":"2023-10-24T01:38:48.087132Z","iopub.status.idle":"2023-10-24T01:38:48.096336Z","shell.execute_reply.started":"2023-10-24T01:38:48.087088Z","shell.execute_reply":"2023-10-24T01:38:48.095129Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# !pip install datasets\n# !pip install transformers \n!pip install rouge_score\n# !pip install --upgrade accelerate\n!pip install transformers==4.28.0","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:39:00.367706Z","iopub.execute_input":"2023-10-24T01:39:00.368565Z","iopub.status.idle":"2023-10-24T01:39:38.685933Z","shell.execute_reply.started":"2023-10-24T01:39:00.368530Z","shell.execute_reply":"2023-10-24T01:39:38.684795Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=e10e1e7f00e4afc9db8861a521e99b22ec45659b03b6bcaddaf91a0018ccae89\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting transformers==4.28.0\n  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (0.13.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (3.11.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (1.23.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (4.64.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (2023.3.23)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (0.13.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (2.28.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (1.26.15)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.28.1\n    Uninstalling transformers-4.28.1:\n      Successfully uninstalled transformers-4.28.1\nSuccessfully installed transformers-4.28.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport multiprocessing as mp\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import io, transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\nfrom transformers import VisionEncoderDecoderModel , ViTFeatureExtractor\nfrom transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\nif torch.cuda.is_available():    \n\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:39:57.759407Z","iopub.execute_input":"2023-10-24T01:39:57.760131Z","iopub.status.idle":"2023-10-24T01:40:19.051265Z","shell.execute_reply.started":"2023-10-24T01:39:57.760087Z","shell.execute_reply":"2023-10-24T01:40:19.050271Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport multiprocessing as mp\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import io, transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\nfrom transformers import VisionEncoderDecoderModel , ViTFeatureExtractor\nfrom transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nclass config : \n    ENCODER = \"google/vit-base-patch16-224\"\n    DECODER = \"gpt2\"\n    TRAIN_BATCH_SIZE = 2\n   \n    LR = 5e-5\n    SEED = 42\n    MAX_LEN = 71\n    SUMMARY_LEN = 20\n    WEIGHT_DECAY = 0.01\n    MEAN = (0.485, 0.456, 0.406)\n    STD = (0.229, 0.224, 0.225)\n    TRAIN_PCT = 0.95\n    NUM_WORKERS = mp.cpu_count()\n    EPOCHS = 3\n    IMG_SIZE = (224,224)\n    LABEL_MASK = -100\n    TOP_K = 1000\n    TOP_P = 0.95\n\n\ndef build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n    return outputs\nAutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n\nrouge = datasets.load_metric(\"rouge\")\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    # all unnecessary tokens are removed\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n\n    return {\n        \"rouge2_precision\": round(rouge_output.precision, 4),\n        \"rouge2_recall\": round(rouge_output.recall, 4),\n        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n    }\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained(config.ENCODER)\ntokenizer = AutoTokenizer.from_pretrained(config.DECODER)\ntokenizer.pad_token = tokenizer.unk_token\n\ntransforms = transforms.Compose(\n    [\n        transforms.Resize(config.IMG_SIZE), \n        transforms.ToTensor(),\n    ]\n)\n\nclass ImgDataset(Dataset):\n    def __init__(self, df, root_dir, tokenizer, feature_extractor, transform=None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        caption = self.df.caption.iloc[idx]\n        image = self.df.image.iloc[idx]\n        img_path = os.path.join(self.root_dir, image)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n\n        captions = self.tokenizer(\n            caption,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        ).input_ids.squeeze()\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": captions}\n        return encoding\n\n\ntrain_dataset = ImgDataset(new_df, root_dir= \"/content/dataset/images/images_normalized/\" , tokenizer=tokenizer, feature_extractor=feature_extractor, transform=transforms)\n\nval_dataset = ImgDataset(val_df , root_dir = \"/content/dataset/images/images_normalized/\" ,tokenizer=tokenizer,feature_extractor = feature_extractor , transform  = transforms)\n\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(config.ENCODER, config.DECODER)\n\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n# set beam search parameters\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.max_length = 128\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4\n\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='VIT_large_gpt2',\n    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n   \n    predict_with_generate=True,\n    evaluation_strategy=\"epoch\",\n    do_train=True,\n    do_eval=True,\n    logging_steps=1024,  \n    save_steps=1500,\n    eval_steps=800,\n    warmup_steps=5000,\n    max_steps=1500,\n    overwrite_output_dir=True,\n    save_total_limit=3,\n    fp16=True,\n)\n\ndef compute_loss(model, inputs):\n    labels = inputs[\"labels\"]\n    decoder_input_ids = labels[:, :-1].contiguous()\n    labels = labels[:, 1:].clone()\n    attention_mask = decoder_input_ids.ne(tokenizer.pad_token_id).float()\n\n    outputs = model(inputs[\"pixel_values\"], decoder_input_ids=decoder_input_ids, attention_mask=attention_mask, labels=labels)\n    lm_logits = outputs.logits\n    loss_fct = nn.CrossEntropyLoss(ignore_index=config.LABEL_MASK)\n    loss = loss_fct(lm_logits.view(-1, model.config.vocab_size), labels.view(-1))\n    return loss\n\n\n# instantiate trainer\n","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:40:49.142463Z","iopub.execute_input":"2023-10-24T01:40:49.142901Z","iopub.status.idle":"2023-10-24T01:41:01.760375Z","shell.execute_reply.started":"2023-10-24T01:40:49.142861Z","shell.execute_reply":"2023-10-24T01:41:01.759519Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e021d287b13b4489a8d8adf94f8c15f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933394b5b27247eb8b528e1bed3cd875"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f658a03754d64ca9b26c22b55f65a3d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40bdeffdfd3b42098bbf14992a376c3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a9dd9edaa2c43bcb24e6e86120bd1a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d8a9bd795f49efba51fcc8bf6b2479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae1c1674e7d4206ba53de53646dce44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac7cce1c4f742f8abd167a7f1e86fd8"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fc25af7ca054678a72fc0a05fe48950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a56f1fe259c942da837b32b1cccbd4bb"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = VisionEncoderDecoderModel.from_pretrained('/kaggle/working/VIT_large_gpt2')\n\ntrainer = Seq2SeqTrainer(\n    tokenizer=feature_extractor,\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-24T01:41:34.998413Z","iopub.execute_input":"2023-10-24T01:41:34.999046Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='47' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  47/1500 00:53 < 28:43, 0.84 it/s, Epoch 0.04/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model('VIT_large_gpt_v3')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:16:48.988573Z","iopub.execute_input":"2023-05-21T21:16:48.988946Z","iopub.status.idle":"2023-05-21T21:16:51.014315Z","shell.execute_reply.started":"2023-05-21T21:16:48.988916Z","shell.execute_reply":"2023-05-21T21:16:51.013043Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"eval_results = trainer.evaluate(eval_dataset=val_dataset)\n\nprint(eval_results)\n\n  ","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:47:12.794447Z","iopub.execute_input":"2023-05-21T17:47:12.795002Z","iopub.status.idle":"2023-05-21T17:58:47.365894Z","shell.execute_reply.started":"2023-05-21T17:47:12.794949Z","shell.execute_reply":"2023-05-21T17:58:47.364850Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"{'eval_loss': 0.06210165470838547, 'eval_rouge2_precision': 0.2012, 'eval_rouge2_recall': 0.1808, 'eval_rouge2_fmeasure': 0.1827, 'eval_runtime': 694.5637, 'eval_samples_per_second': 1.863, 'eval_steps_per_second': 0.117}\n","output_type":"stream"}]},{"cell_type":"code","source":"import tqdm \n# model = VisionEncoderDecoderModel.from_pretrained('/kaggle/working/VIT_large_gpt_v3')\npredicted_captions = [] \nfor i in tqdm.tqdm( val_df['image']):\n    \n    img =  Image.open(i).convert(\"RGB\")\n    caption = tokenizer.decode(model.generate(feature_extractor(img, return_tensors=\"pt\").pixel_values.to(\"cuda\"))[0],skip_special_tokens=True)\n    predicted_captions.append(caption)\nprint(len(predicted_captions))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T22:21:08.394900Z","iopub.execute_input":"2023-05-21T22:21:08.396104Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 25%|██▍       | 323/1294 [06:43<18:59,  1.17s/it]","output_type":"stream"}]},{"cell_type":"code","source":"img =  Image.open(new_df['image'][1]).convert(\"RGB\")\ncaption = tokenizer.decode(model.generate(feature_extractor(img, return_tensors=\"pt\").pixel_values.to(\"cuda\"))[0],skip_special_tokens=True)\nprint(caption)\nprint()\nimg = img.resize((250,250))\nimg ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n\n# Assuming you have a list of predicted captions and a list of ground truth captions\ngenerated_captions = predicted_captions\nground_truth_captions = val_df['caption'].values\n# Convert the caption lists into the format expected by nltk\nground_truth_captions = [[caption.split() for caption in captions] for captions in ground_truth_captions]\ngenerated_captions = [caption.split() for caption in generated_captions]\n\n\n# Define the smoothing function to use\nsmoothie = SmoothingFunction().method4\n\n# Compute the BLEU score with smoothing\nweights = (0.25, 0.25, 0.25, 0.25)  # equal weights for 1-4 gram BLEU scores\nscore = corpus_bleu(ground_truth_captions, predicted_captions,weights =weights)\nprint(f'The BELU Score Is: {score}')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:11:54.334200Z","iopub.execute_input":"2023-05-21T21:11:54.334748Z","iopub.status.idle":"2023-05-21T21:13:24.932962Z","shell.execute_reply.started":"2023-05-21T21:11:54.334713Z","shell.execute_reply":"2023-05-21T21:13:24.931984Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"The BELU Score Is: 0.5694104045583323\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\n\n# Load and preprocess the image\nimage = Image.open(\"path/to/image.jpg\")\n# Apply any necessary image preprocessing steps\n\n# Preprocess the image using the feature extractor\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n# Generate the caption\noutputs = model.generate(inputs['pixel_values'])\ncaption = tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{},"execution_count":null,"outputs":[]}]}